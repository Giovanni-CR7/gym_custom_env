# python train_grid_world_cover.py train   # para treinar
# python train_grid_world_cover.py test    # para testar

import sys
import gymnasium as gym
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch as th
import torch.nn as nn
import os

from stable_baselines3 import PPO
from stable_baselines3.common.logger import configure
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from gymnasium_env.grid_world_cover_render import GridWorldCoverRenderEnv 

# --- Custom CNN ---
class CustomCnn(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=128):
        super().__init__(observation_space, features_dim)
        c, h, w = observation_space.shape
        # ajuste leve na CNN para grid maior 
        self.cnn = nn.Sequential(
            nn.Conv2d(c, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), # Camada extra
            nn.ReLU(),
            nn.Flatten()
        )
        with th.no_grad():
            sample = th.as_tensor(np.zeros((1, c, h, w), dtype=np.float32))
            n_flatten = self.cnn(sample).shape[1]
        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())

    def forward(self, observations: th.Tensor) -> th.Tensor:
        # Normaliza√ß√£o opcional pode ajudar CNNs
        # observations = observations / 255.0 # Descomente se sua obs n√£o for 0-1
        x = self.cnn(observations)
        return self.linear(x)

# --- Wrapper de Cobertura ---
class CoverageWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.env = env
        self.grid_size = env.observation_space.shape[1]
        # Pega o max_steps definido no ambiente base
        self.max_steps = env.max_steps if hasattr(env, "max_steps") else self.grid_size * self.grid_size * 4
        self.visited = set()
        self.obstacles = set()
        self.current_step = 0
        self.total_coverable_cells = self.grid_size * self.grid_size

        self.observation_space = env.observation_space
        self.action_space = env.action_space
        print(f"[Wrapper] Grid Size: {self.grid_size}, Max Steps: {self.max_steps}")


    def _get_obs(self, obs):
        return obs

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.obstacles = set(info.get("obstacles", []))
        self.total_coverable_cells = self.grid_size * self.grid_size - len(self.obstacles)
        
        visited_mask = obs[3]
        self.visited = set()
        for i in range(self.grid_size):
            for j in range(self.grid_size):
                if visited_mask[i, j] > 0.5:
                    self.visited.add((i, j))
        
        agent_channel = obs[2]
        # Garante que funciona mesmo se o agente come√ßar fora do grid (n√£o deve acontecer)
        agent_pos_search = np.argwhere(agent_channel == 1.0)
        if agent_pos_search.shape[0] > 0:
             ai, aj = agent_pos_search[0]
             self.visited.add((int(ai), int(aj)))
        
        self.current_step = 0
        # print(f"[Wrapper Reset] Coverable: {self.total_coverable_cells}, Initial Visited: {len(self.visited)}") # Debug
        return self._get_obs(obs), info

    def step(self, action):
        obs, base_reward, terminated, truncated, info = self.env.step(action)
        self.current_step += 1

        agent_channel = obs[2]
        pos = np.argwhere(agent_channel == 1.0)
        if pos.shape[0] == 0:
             try: # Fallback se _agent_location existir
                 ai, aj = tuple(self.env.unwrapped._agent_location)
             except Exception: # √öltimo recurso
                 ai, aj = -1, -1 # Indica posi√ß√£o inv√°lida
        else:
            ai, aj = int(pos[0,0]), int(pos[0,1])
        
        agent_pos = (ai, aj) if ai != -1 else None # Posi√ß√£o atual v√°lida?

        hit_obstacle = info.get("hit_obstacle", False)

        # --- RECOMPENSA "TRATOR" ---
        reward = -0.01  # Custo de passo

        if hit_obstacle:
            reward += 0.0 # NENHUMA penalidade por bater
        # Se n√£o bateu E a posi√ß√£o √© v√°lida
        elif agent_pos is not None:
            if agent_pos not in self.visited:
                self.visited.add(agent_pos)
                reward += 1.0 # Recompensa por c√©lula nova
            else:
                reward += -0.1 # Penalidade por revisitar

        # B√¥nus terminal por cobertura total
        if len(self.visited) >= self.total_coverable_cells:
            terminated = True
            reward += 50.0
            # print(f"[Wrapper Step {self.current_step}] Coverage Complete! Reward: {reward}") # Debug

        # Truncamento por limite de passos (sem penalidade)
        if self.current_step >= self.max_steps:
            truncated = True

        # Atualiza a observa√ß√£o (canal visitado) se a posi√ß√£o for v√°lida
        if agent_pos is not None:
             obs[3, ai, aj] = 1.0

        return self._get_obs(obs), float(reward), bool(terminated), bool(truncated), info

# --- Factory do Ambiente ---
def make_env(render_mode=None, size=20, log_dir=None, num_obstacles=40, seed=None):
    # Passa o tamanho correto para o ambiente base
    env = GridWorldCoverRenderEnv(size=size, render_mode=render_mode, num_obstacles=num_obstacles, seed=seed)
    env = CoverageWrapper(env)
    if log_dir:
        os.makedirs(log_dir, exist_ok=True) # Cria o diret√≥rio se n√£o existir
        env = Monitor(env, log_dir)
    return env

# --- Fun√ß√£o de Plotagem ---
def plot_results(log_dir):
    try:
        monitor_files = [f for f in os.listdir(log_dir) if f.endswith("monitor.csv")]
        if not monitor_files:
            print(f"‚ö†Ô∏è Nenhum arquivo monitor.csv encontrado em {log_dir}")
            return
        
        filepath = os.path.join(log_dir, monitor_files[0])
        print(f"üìä Gerando gr√°fico a partir de '{filepath}'...")
        data = pd.read_csv(filepath, skiprows=1)
        
        if data.empty or 'r' not in data.columns:
             print("‚ö†Ô∏è Arquivo monitor.csv est√° vazio ou n√£o cont√©m a coluna 'r'.")
             return

        window_size = 100
        # Certifica que temos dados suficientes para a janela
        if len(data['r']) >= window_size:
            rolling_avg = data['r'].rolling(window=window_size).mean()
        else:
            rolling_avg = data['r'].rolling(window=len(data['r'])).mean() # M√©dia de tudo se for pouco

        plt.figure(figsize=(12, 6))
        plt.title("Evolu√ß√£o da Recompensa M√©dia por Epis√≥dio")
        plt.xlabel("Epis√≥dios")
        plt.ylabel("Recompensa M√©dia (M√≥vel)")
        plt.plot(data.index, data['r'], 'b.', alpha=0.2, label='Recompensa por Epis√≥dio')
        plt.plot(data.index, rolling_avg, 'r-', linewidth=2, label=f'M√©dia M√≥vel ({window_size} epis√≥dios)')
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"‚ö†Ô∏è N√£o foi poss√≠vel gerar o gr√°fico: {e}")

# --- Script Principal ---
if __name__ == "__main__":
    train_mode = True if len(sys.argv) > 1 and sys.argv[1] == "train" else False

    gym.register(
        id="gymnasium_env/GridWorld-Coverage-v0",
        entry_point=GridWorldCoverRenderEnv,
    )

    # --- PAR√ÇMETROS PARA 20x20 ---
    GRID_SIZE = 20
    NUM_OBSTACLES = 40
    LOG_DIR = "log/ppo_coverage_20x20"
    MODEL_SAVE_PATH = "data/ppo_coverage_20x20.zip" # Nome do modelo salvo
    TOTAL_TIMESTEPS = 17_000_000 # Comece com 10M, ajuste se necess√°rio

    if train_mode:
        print(f"--- Iniciando Treinamento: Grid {GRID_SIZE}x{GRID_SIZE}, Obst√°culos: {NUM_OBSTACLES}, Passos: {TOTAL_TIMESTEPS} ---")
        env = make_env(render_mode=None, size=GRID_SIZE, log_dir=LOG_DIR, num_obstacles=NUM_OBSTACLES, seed=0)

        policy_kwargs = dict(
            features_extractor_class=CustomCnn,
            features_extractor_kwargs=dict(features_dim=128),
            # Redes um pouco maiores podem ajudar em grids maiores
            net_arch=dict(pi=[256, 128], vf=[256, 128])
        )

        model = PPO(
            "CnnPolicy",
            env,
            verbose=1,
            device="cuda", # ou cuda para a gpu nvidia
            policy_kwargs=policy_kwargs,
            ent_coef=0.02,
            learning_rate=3e-4, # Pode precisar diminuir se inst√°vel (ex: 1e-4)
            gamma=0.999,
            n_steps=4096, # Horizonte maior
            batch_size=128, # Batch size maior pode ajudar com n_steps maior
            n_epochs=10, # Padr√£o do PPO
            clip_range=0.2 # Padr√£o do PPO
        )

        # Configura logger (sobrescreve se j√° existir)
        logger = configure(LOG_DIR, ["stdout", "csv", "tensorboard"])
        model.set_logger(logger)

        try:
            model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True)
        except KeyboardInterrupt:
            print("\nüõë Treinamento interrompido pelo usu√°rio.")
        finally:
            # Garante que o diret√≥rio de dados existe
            os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
            model.save(MODEL_SAVE_PATH)
            print(f"‚úÖ Modelo salvo em {MODEL_SAVE_PATH}")
            plot_results(LOG_DIR) # Tenta plotar no final

    # --- Bloco de Teste ---
    print(f"üìÇ Carregando modelo salvo de '{MODEL_SAVE_PATH}'...")
    # Verifica se o modelo existe antes de carregar
    if not os.path.exists(MODEL_SAVE_PATH):
         print(f"‚ùå Erro: Arquivo do modelo n√£o encontrado em '{MODEL_SAVE_PATH}'. Execute o treino primeiro.")
         sys.exit(1)
         
    model = PPO.load(MODEL_SAVE_PATH)

    # Cria ambiente para teste (com renderiza√ß√£o)
    env = make_env(render_mode="human", size=GRID_SIZE, num_obstacles=NUM_OBSTACLES)

    for i in range(5): # Testa por 5 epis√≥dios
        print(f"\n--- Iniciando Teste {i+1} ---")
        obs, _ = env.reset()
        done, truncated = False, False
        steps = 0
        total_reward = 0
        visited_in_ep = set() # Track visited for this episode specifically in test

        # Adiciona a posi√ß√£o inicial ao conjunto visitado do teste
        agent_channel = obs[2]
        agent_pos_search = np.argwhere(agent_channel == 1.0)
        if agent_pos_search.shape[0] > 0:
             ai, aj = agent_pos_search[0]
             visited_in_ep.add((int(ai), int(aj)))
             
        while not (done or truncated):
            action, _ = model.predict(obs, deterministic=True)
            a = int(action.item()) if isinstance(action, np.ndarray) else int(action)

            obs, reward, done, truncated, info = env.step(a)
            total_reward += reward

            # Atualiza visitados para este epis√≥dio de teste
            agent_channel = obs[2]
            pos = np.argwhere(agent_channel == 1.0)
            if pos.shape[0] > 0:
                 ai, aj = int(pos[0,0]), int(pos[0,1])
                 visited_in_ep.add((ai, aj))

            # Atualiza a renderiza√ß√£o (se o wrapper n√£o fizer isso automaticamente)
            # Acessa o ambiente base atrav√©s de env.env se estiver usando Monitor + Wrapper
            try:
                 # Tenta acessar o m√©todo do ambiente base para renderizar 'visited'
                 env.unwrapped.set_visited_cells(visited_in_ep)
            except AttributeError:
                 # Fallback se a estrutura for diferente
                 try:
                      env.env.unwrapped.set_visited_cells(visited_in_ep)
                 except AttributeError:
                     pass # Renderiza√ß√£o pode n√£o mostrar visitados corretamente
                     
            steps += 1
            env.render() # For√ßa a renderiza√ß√£o a cada passo no modo 'human'

        print(f"Epis√≥dio finalizado em {steps} passos. Terminated={done}, Truncated={truncated}. Recompensa total: {total_reward:.2f}")
        
        # Pega informa√ß√µes de cobertura do ambiente base
        try:
             unwrapped_env = env.unwrapped # Tenta acessar diretamente
             # Recalcula obst√°culos e total coberto no teste para garantir
             obstacles_test = unwrapped_env._obstacle_locations
             total_cover_test = unwrapped_env.size * unwrapped_env.size - len(obstacles_test)
             visited_cnt = len(visited_in_ep)
             print(f"Cobertura: {visited_cnt} de {total_cover_test} c√©lulas acess√≠veis ({visited_cnt/total_cover_test*100:.1f}%).")
        except AttributeError:
             print("N√£o foi poss√≠vel obter informa√ß√µes detalhadas de cobertura do ambiente.")


    env.close()
    print("\n--- Testes Conclu√≠dos ---")

