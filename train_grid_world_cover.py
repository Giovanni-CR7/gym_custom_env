# python train_grid_world_cover.py train  # para treinar
# python train_grid_world_cover.py test   # para testar

import sys
import gymnasium as gym
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch as th
import torch.nn as nn
import os
import collections # Importado para uso no ambiente, mas bom ter aqui

from stable_baselines3 import PPO
from stable_baselines3.common.logger import configure
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
# Garanta que o Python consiga encontrar este m√≥dulo (pode precisar ajustar PYTHONPATH ou estrutura)
from gymnasium_env.grid_world_cover_render import GridWorldCoverRenderEnv

# --- Custom CNN ---
class CustomCnn(BaseFeaturesExtractor):
    """ CNN customizada para extrair features da observa√ß√£o em grid. """
    def __init__(self, observation_space, features_dim=256): # Aumentado features_dim
        super().__init__(observation_space, features_dim)
        c, h, w = observation_space.shape
        # Rede um pouco mais profunda pode ajudar com 20x20
        self.cnn = nn.Sequential(
            nn.Conv2d(c, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # Camada extra
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), # Outra camada extra
            nn.ReLU(),
            nn.Flatten()
        )
        # Calcula o tamanho da sa√≠da achatada da CNN
        with th.no_grad():
            sample = th.as_tensor(np.zeros((1, c, h, w), dtype=np.float32))
            n_flatten = self.cnn(sample).shape[1]
        # Camada linear final para a dimens√£o de features desejada
        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())

    def forward(self, observations: th.Tensor) -> th.Tensor:
        # Normaliza√ß√£o opcional (se a observa√ß√£o n√£o estiver entre 0 e 1)
        # observations = observations / 1.0 # Ajuste se necess√°rio
        x = self.cnn(observations)
        return self.linear(x)

# --- Wrapper de Cobertura com Recompensa "Trator" ---
class CoverageWrapper(gym.Wrapper):
    """ Aplica a l√≥gica de recompensa focada em efici√™ncia e cobertura. """
    def __init__(self, env):
        super().__init__(env)
        self.env = env
        self.grid_size = env.observation_space.shape[1]
        # Pega o max_steps do ambiente base (que deve ser size*size*4 = 1600)
        self.max_steps = env.max_steps if hasattr(env, "max_steps") else int(self.grid_size * self.grid_size * 4)
        self.visited = set()
        self.obstacles = set()
        self.current_step = 0
        self.total_coverable_cells = self.grid_size * self.grid_size # Ser√° ajustado no reset

        self.observation_space = env.observation_space
        self.action_space = env.action_space
        print(f"[Wrapper] Inicializado. Grid Size: {self.grid_size}, Max Steps: {self.max_steps}")

    def _get_obs(self, obs):
        """ Retorna a observa√ß√£o como est√° (formato de canais). """
        return obs

    def reset(self, **kwargs):
        """ Reseta o wrapper e o ambiente interno. """
        obs, info = self.env.reset(**kwargs)
        # Pega os obst√°culos reais do mapa gerado (importante ap√≥s a valida√ß√£o no env.reset)
        self.obstacles = set(info.get("obstacles", []))
        # Calcula c√©lulas *realmente* cobr√≠veis
        self.total_coverable_cells = self.grid_size * self.grid_size - len(self.obstacles)

        # Inicializa conjunto de visitados (agente come√ßa em uma c√©lula)
        self.visited = set()
        agent_channel = obs[2]
        agent_pos_search = np.argwhere(agent_channel == 1.0)
        if agent_pos_search.shape[0] > 0:
             ai, aj = agent_pos_search[0]
             self.visited.add((int(ai), int(aj)))
        else:
             print("AVISO [Wrapper Reset]: Posi√ß√£o inicial do agente n√£o encontrada na observa√ß√£o!")
             # Tenta pegar do info como fallback
             try:
                  start_loc = info.get("agent_location")
                  if start_loc: self.visited.add(start_loc)
             except Exception: pass


        self.current_step = 0
        # print(f"[Wrapper Reset] Coverable: {self.total_coverable_cells}, Initial Visited: {len(self.visited)}") # Debug
        return self._get_obs(obs), info

    def step(self, action):
        """ Executa um passo, calcula a recompensa e atualiza o estado. """
        obs, base_reward, terminated, truncated, info = self.env.step(action)
        self.current_step += 1

        # Pega a posi√ß√£o atual do agente a partir da observa√ß√£o
        agent_channel = obs[2]
        pos = np.argwhere(agent_channel == 1.0)
        agent_pos = None # Reset
        if pos.shape[0] > 0:
            ai, aj = int(pos[0,0]), int(pos[0,1])
            agent_pos = (ai, aj)
        else: # Fallback se n√£o encontrar na obs (n√£o deveria acontecer)
             try: agent_pos = tuple(self.env.unwrapped._agent_location)
             except Exception: pass

        hit_obstacle = info.get("hit_obstacle", False)

        # --- RECOMPENSA "TRATOR" COM ALTO CUSTO DE PASSO ---
        reward = -0.2  # Custo alto por passo para for√ßar efici√™ncia

        # Nenhuma penalidade direta por bater em obst√°culo
        if hit_obstacle:
            pass # Equivalente a reward += 0.0

        # Se n√£o bateu e a posi√ß√£o √© v√°lida
        elif agent_pos is not None:
            if agent_pos not in self.visited:
                self.visited.add(agent_pos)
                reward += 1.0 # Recompensa por c√©lula nova
            else:
                reward += -0.1 # Penalidade por revisitar

        # B√¥nus terminal por cobertura total (usa o total_coverable_cells real)
        if len(self.visited) >= self.total_coverable_cells:
            terminated = True
            reward += 50.0
            # print(f"[Wrapper Step {self.current_step}] Coverage Complete! Visited: {len(self.visited)}, Total: {self.total_coverable_cells}. Reward: {reward}") # Debug

        # Truncamento por limite de passos (sem penalidade adicional)
        if not terminated and self.current_step >= self.max_steps:
            truncated = True
            # print(f"[Wrapper Step {self.current_step}] Truncated. Visited: {len(self.visited)}, Total: {self.total_coverable_cells}") # Debug


        # Atualiza o canal 'visitados' na observa√ß√£o para consist√™ncia (√∫til para o agente e render)
        if agent_pos is not None:
             obs[3, agent_pos[0], agent_pos[1]] = 1.0

        return self._get_obs(obs), float(reward), bool(terminated), bool(truncated), info

# --- Factory do Ambiente ---
def make_env(render_mode=None, size=20, log_dir=None, num_obstacles=40, seed=None):
    """ Cria e envolve o ambiente. """
    env = GridWorldCoverRenderEnv(size=size, render_mode=render_mode, num_obstacles=num_obstacles, seed=seed)
    env = CoverageWrapper(env) # Aplica o wrapper de recompensa
    if log_dir:
        os.makedirs(log_dir, exist_ok=True) # Cria o diret√≥rio se n√£o existir
        # Envolve com Monitor para salvar logs em CSV
        env = Monitor(env, log_dir) # 'is_success' √© opcional
    return env

# --- Fun√ß√£o de Plotagem ---
def plot_results(log_dir):
    """ Plota a recompensa m√©dia m√≥vel a partir do monitor.csv. """
    try:
        monitor_files = [f for f in os.listdir(log_dir) if f.endswith("monitor.csv")]
        if not monitor_files:
            print(f"‚ö†Ô∏è Nenhum arquivo monitor.csv encontrado em {log_dir}")
            return

        filepath = os.path.join(log_dir, monitor_files[0])
        print(f"üìä Gerando gr√°fico a partir de '{filepath}'...")
        # Tenta carregar, tratando erros se o arquivo estiver corrompido ou mal formatado
        try:
            data = pd.read_csv(filepath, skiprows=1) # Pula a linha de cabe√ßalho JSON
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao ler {filepath}: {e}. O arquivo pode estar incompleto.")
            return

        if data.empty or 'r' not in data.columns:
             print(f"‚ö†Ô∏è Arquivo {filepath} est√° vazio ou n√£o cont√©m a coluna de recompensa 'r'.")
             return

        window_size = 100 # Janela da m√©dia m√≥vel
        # Calcula m√©dia m√≥vel, tratando casos com poucos dados
        if len(data['r']) >= window_size:
            rolling_avg = data['r'].rolling(window=window_size).mean()
        elif len(data['r']) > 0:
             rolling_avg = data['r'].rolling(window=len(data['r'])).mean()
        else:
             rolling_avg = pd.Series() # S√©rie vazia se n√£o houver dados

        plt.figure(figsize=(12, 6))
        plt.title("Evolu√ß√£o da Recompensa M√©dia por Epis√≥dio")
        plt.xlabel("Epis√≥dios")
        plt.ylabel("Recompensa M√©dia (M√≥vel)")
        plt.plot(data.index, data['r'], 'b.', alpha=0.15, label='Recompensa por Epis√≥dio') # Pontos mais transparentes
        if not rolling_avg.empty:
             plt.plot(data.index, rolling_avg, 'r-', linewidth=2, label=f'M√©dia M√≥vel ({window_size} epis√≥dios)')
        plt.grid(True)
        plt.legend()
        plt.tight_layout() # Ajusta o layout para evitar sobreposi√ß√£o
        plt.show() # Mostra o gr√°fico
    except Exception as e:
        print(f"‚ö†Ô∏è N√£o foi poss√≠vel gerar o gr√°fico completo: {e}")

# --- Script Principal (Treino e Teste) ---
if __name__ == "__main__":
    # Verifica se o primeiro argumento √© 'train' ou 'test'
    mode = sys.argv[1] if len(sys.argv) > 1 else "test" # Default para testar se nenhum argumento for dado

    # Registra o ambiente no Gymnasium (se ainda n√£o estiver registrado)
    try:
        gym.register(
            id="gymnasium_env/GridWorld-Coverage-v0",
            entry_point="gymnasium_env.grid_world_cover_render:GridWorldCoverRenderEnv",
        )
    except gym.error.Error as e:
        # print(f"Ambiente j√° registrado: {e}") # Debug opcional
        pass

    # --- PAR√ÇMETROS GLOBAIS ---
    GRID_SIZE = 20
    NUM_OBSTACLES = 40
    LOG_DIR = f"log/ppo_coverage_{GRID_SIZE}x{GRID_SIZE}"
    MODEL_SAVE_PATH = f"data/ppo_coverage_{GRID_SIZE}x{GRID_SIZE}.zip"
    TOTAL_TIMESTEPS = 17_000_000 # Or√ßamento total de treino

    # --- MODO DE TREINAMENTO ---
    if mode == "train":
        print(f"--- Iniciando Treinamento: Grid {GRID_SIZE}x{GRID_SIZE}, Obst√°culos: {NUM_OBSTACLES}, Passos: {TOTAL_TIMESTEPS} ---")
        # Cria o ambiente para treino (sem renderiza√ß√£o, com Monitor)
        env = make_env(render_mode=None, size=GRID_SIZE, log_dir=LOG_DIR, num_obstacles=NUM_OBSTACLES, seed=0)

        # Configura√ß√£o da rede neural (Pol√≠tica e Fun√ß√£o de Valor)
        policy_kwargs = dict(
            features_extractor_class=CustomCnn,
            features_extractor_kwargs=dict(features_dim=256), # Aumentado
            net_arch=dict(pi=[256, 128], vf=[256, 128])    # Aumentado
        )

        # Configura√ß√£o do Algoritmo PPO
        model = PPO(
            "CnnPolicy",           # Usa a pol√≠tica baseada em CNN
            env,                   # O ambiente envolvido
            verbose=1,             # Imprime informa√ß√µes de progresso
            device="cpu",          # Mude para "cuda" se tiver GPU NVIDIA configurada
            policy_kwargs=policy_kwargs, # Arquitetura da rede customizada
            ent_coef=0.01,         # Coeficiente de entropia (explora√ß√£o) - pode ajustar (ex: 0.005)
            learning_rate=5e-5,    # Taxa de aprendizado REDUZIDA
            gamma=0.999,           # Fator de desconto ALTO (vis√£o de longo prazo)
            n_steps=4096,          # Horizonte de coleta MAIOR
            batch_size=128,        # Tamanho do batch para atualiza√ß√£o
            n_epochs=20,           # N√∫mero de √©pocas de otimiza√ß√£o por coleta
            clip_range=0.2,        # Clipping do PPO (padr√£o)
            tensorboard_log=LOG_DIR # Diret√≥rio para logs do TensorBoard
        )

        print("Configura√ß√£o do Modelo PPO:")
        print(f"  Learning Rate: {model.learning_rate}")
        print(f"  N Steps: {model.n_steps}")
        print(f"  Batch Size: {model.batch_size}")
        print(f"  N Epochs: {model.n_epochs}")
        print(f"  Gamma: {model.gamma}")
        print(f"  Ent Coef: {model.ent_coef}")
        print(f"  Clip Range: {model.clip_range}")
        print(f"  Device: {model.device}")
        
        # O Monitor j√° configura o logger CSV, aqui configuramos stdout e TensorBoard
        # logger = configure(LOG_DIR, ["stdout", "tensorboard"]) # O Monitor j√° lida com CSV
        # model.set_logger(logger) # set_logger √© mais usado se n√£o usar Monitor ou quiser formatters customizados

        try:
            # Inicia o treinamento
            model.learn(
                total_timesteps=TOTAL_TIMESTEPS,
                log_interval=10, # Loga estat√≠sticas a cada 10 atualiza√ß√µes
                progress_bar=True # Mostra barra de progresso
            )
        except KeyboardInterrupt:
            print("\nüõë Treinamento interrompido pelo usu√°rio.")
        finally:
            # Garante que o diret√≥rio de dados existe
            os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
            # Salva o modelo treinado
            model.save(MODEL_SAVE_PATH)
            print(f"‚úÖ Modelo salvo em {MODEL_SAVE_PATH}")
            # Tenta plotar o gr√°fico no final
            plot_results(LOG_DIR)

    # --- MODO DE TESTE ---
    elif mode == "test":
        print(f"--- Iniciando Modo de Teste: Grid {GRID_SIZE}x{GRID_SIZE}, Obst√°culos: {NUM_OBSTACLES} ---")
        print(f"üìÇ Carregando modelo treinado de '{MODEL_SAVE_PATH}'...")

        # Verifica se o arquivo do modelo existe
        if not os.path.exists(MODEL_SAVE_PATH):
             print(f"‚ùå Erro: Arquivo do modelo n√£o encontrado em '{MODEL_SAVE_PATH}'. Execute o treino primeiro ('python {sys.argv[0]} train').")
             sys.exit(1) # Termina o script se o modelo n√£o for encontrado

        # Carrega o modelo treinado
        # N√£o precisa passar 'env' aqui se s√≥ for usar para predict, mas √© boa pr√°tica se for adaptar depois
        model = PPO.load(MODEL_SAVE_PATH)
        print("‚úÖ Modelo carregado.")

        # Cria o ambiente para teste (com renderiza√ß√£o "human" e SEM Monitor)
        # Usar uma seed diferente no teste pode ser interessante para ver generaliza√ß√£o
        env = make_env(render_mode="human", size=GRID_SIZE, num_obstacles=NUM_OBSTACLES, log_dir=None, seed=np.random.randint(1000))

        num_test_episodes = 5
        for i in range(num_test_episodes):
            print(f"\n--- Iniciando Teste {i+1}/{num_test_episodes} ---")
            obs, info = env.reset() # Obt√©m a observa√ß√£o e info iniciais
            done, truncated = False, False
            steps = 0
            total_reward = 0
            
            # Pega o set de visitados do wrapper interno para renderizar corretamente
            # Acessa atrav√©s de env.env por causa do Monitor (se estivesse presente)
            # Como removemos o Monitor para teste, o acesso √© direto via env.
            current_visited_for_render = set(env.visited) if hasattr(env, 'visited') else set()

            while not (done or truncated):
                # Pede ao modelo a melhor a√ß√£o (determin√≠stica)
                action, _ = model.predict(obs, deterministic=True)
                # Converte a√ß√£o para int (necess√°rio para o ambiente)
                a = int(action.item()) if isinstance(action, np.ndarray) else int(action)

                # Executa a a√ß√£o no ambiente
                obs, reward, done, truncated, info = env.step(a)
                total_reward += reward
                
                # Atualiza o conjunto de visitados para renderiza√ß√£o a partir do wrapper
                if hasattr(env, 'visited'):
                     current_visited_for_render = set(env.visited)
                
                # Atualiza a renderiza√ß√£o com as c√©lulas visitadas corretas
                try:
                    # Tenta chamar o m√©todo do ambiente base para atualizar o render
                    env.unwrapped.set_visited_cells(current_visited_for_render)
                except AttributeError:
                     print("Aviso: N√£o foi poss√≠vel chamar set_visited_cells no ambiente unwrapped.")
                     pass # Renderiza√ß√£o pode n√£o mostrar visitados corretamente
                         
                steps += 1
                env.render() # For√ßa a renderiza√ß√£o (j√° deve ser chamada no step do env base)

            print(f"Epis√≥dio finalizado em {steps} passos.")
            print(f"  Terminated (Cobertura Completa): {done}")
            print(f"  Truncated (Limite de Passos): {truncated}")
            print(f"  Recompensa Total: {total_reward:.2f}")

            # Calcula e exibe a cobertura final
            try:
                 # Pega informa√ß√µes do ambiente base diretamente (sem Monitor no teste)
                 unwrapped_env = env.unwrapped
                 # Pega obst√°culos e calcula total coberto deste epis√≥dio espec√≠fico
                 obstacles_test = unwrapped_env._obstacle_locations
                 total_cover_test = unwrapped_env.size * unwrapped_env.size - len(obstacles_test)
                 # Usa o conjunto 'visited' final do wrapper para a contagem correta
                 visited_cnt = len(current_visited_for_render)
                 coverage_percent = (visited_cnt / total_cover_test * 100) if total_cover_test > 0 else 0
                 print(f"  Cobertura Final: {visited_cnt} de {total_cover_test} c√©lulas acess√≠veis ({coverage_percent:.1f}%).")
            except Exception as e:
                 print(f"  N√£o foi poss√≠vel calcular a cobertura detalhada: {e}")

        env.close() # Fecha a janela Pygame
        print("\n--- Testes Conclu√≠dos ---")

    else:
        print(f"Modo inv√°lido '{mode}'. Use 'train' ou 'test'.")
        sys.exit(1)

